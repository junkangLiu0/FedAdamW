
# FedAdamW
# Federated Learning Framework README

This repository contains two main federated learning scripts: `new_adamw.py` (for CNN-based models) and `new_llm.py` (for transformer-based models). Below is a comprehensive guide for running experiments and understanding all parameters.

---

## Quick Start

### 1. CNN Training (CIFAR-100)
```bash
python new_adamw.py \
  --alg FedAdamW \
  --lr 3e-4 \
  --data_name CIFAR100 \
  --alpha_value 0.1 \
  --alpha 0.01 \
  --epoch 101 \
  --extname FedAvg_adamw_P \
  --lr_decay 2 \
  --gamma 0.5 \
  --CNN swin_tiny \
  --E 5 \
  --batch_size 16 \
  --gpu 2 \
  --p 1 \
  --num_gpus_per 0.2 \
  --normalization BN \
  --selection 0.05 \
  --print 0 \
  --pre 1 \
  --num_workers 100 \
  --preprint 10 \
  --beta1 0.9 \
  --beta2 0.999 \
  --rho 0.01 \
  --pix 224 \
  --lora 0 \
  --K 50
```

### 2. CNN Training (ResNet-18)
```bash
python new_adamw.py \
  --alg FedAdamW \
  --lr 3e-4 \
  --data_name CIFAR100 \
  --alpha_value 0.1 \
  --alpha 0.001 \
  --epoch 301 \
  --extname FedAvg_adamw_P \
  --lr_decay 2 \
  --gamma 0.5 \
  --CNN resnet18 \
  --E 5 \
  --batch_size 50 \
  --gpu 1 \
  --p 1 \
  --num_gpus_per 0.1 \
  --normalization BN \
  --selection 0.1 \
  --print 0 \
  --pre 1 \
  --num_workers 100 \
  --preprint 10 \
  --beta1 0.9 \
  --beta2 0.999 \
  --rho 0.01 \
  --pix 32 \
  --lora 0 \
  --K 50
```

### 3. Vision Transformer Training
```bash
python new_adamw.py \
  --alg FedAdamW \
  --lr 3e-4 \
  --data_name CIFAR100 \
  --alpha_value 0.1 \
  --alpha 0.001 \
  --epoch 301 \
  --extname FedAvg_adamw_P \
  --lr_decay 2 \
  --gamma 0.5 \
  --CNN deit_tiny \
  --E 5 \
  --batch_size 50 \
  --gpu 2 \
  --p 1 \
  --num_gpus_per 0.1 \
  --normalization BN \
  --selection 0.1 \
  --print 0 \
  --pre 1 \
  --num_workers 100 \
  --preprint 10 \
  --beta1 0.9 \
  --beta2 0.999 \
  --rho 0.01 \
  --pix 32 \
  --lora 0 \
  --K 50
```

---

## Parameter Reference

### Core Federated Learning Parameters
| Parameter | Description |
|-----------|-------------|
| `--alg` | Algorithm choice: `FedAvg`, `FedAdamW`, `FedCM`, `SCAFFOLD`, etc. |
| `--lr` | Client learning rate |
| `--lr_decay` | Learning rate decay strategy (1=exponential, 2=cosine annealing) |
| `--gamma` | Momentum parameter for certain algorithms |
| `--alpha` | Weight decay coefficient for AdamW optimizer |

### Data Parameters
| Parameter | Description |
|-----------|-------------|
| `--data_name` | Dataset: `CIFAR10`, `CIFAR100`, `imagenet`, `QQP`, `MNLI`, etc. |
| `--alpha_value` | Dirichlet distribution parameter for non-IID data splitting (0.1=highly non-IID, 1=IID) |
| `--num_workers` | Total number of clients |
| `--selection` | Fraction of clients selected per round (0.1=10%) |

### Model Parameters
| Parameter | Description |
|-----------|-------------|
| `--CNN` | Model architecture: `resnet18`, `swin_tiny`, `deit_tiny`, `roberta_base` |
| `--pre` | Use pretrained weights (1=True, 0=False) |
| `--normalization` | Normalization type: `BN` (BatchNorm) or `GN` (GroupNorm) |
| `--pix` | Input image size (32 for CIFAR, 224 for ImageNet) |

### Training Parameters
| Parameter | Description |
|-----------|-------------|
| `--epoch` | Total communication rounds |
| `--E` | Local epochs per client |
| `--batch_size` | Client batch size |
| `--K` | Maximum local steps per round (overrides E if smaller) |
| `--p` | Parallelism factor for client updates |

### LoRA Parameters
| Parameter | Description |
|-----------|-------------|
| `--lora` | Enable LoRA fine-tuning (1=True, 0=False) |
| `--r` | LoRA rank |
| `--lora_alpha` | LoRA scaling parameter |

### Optimization Parameters
| Parameter | Description |
|-----------|-------------|
| `--beta1` | Adam optimizer Î²1 parameter |
| `--beta2` | Adam optimizer Î²2 parameter |
| `--rho` | SAM optimizer perturbation radius |
| `--optimizer` | Base optimizer: `SGD` or `AdamW` |

### System Parameters
| Parameter | Description |
|-----------|-------------|
| `--gpu` | GPU device IDs (e.g., "0,1,2") |
| `--num_gpus_per` | GPU fraction per client (0.2=20% of a GPU) |
| `--print` | Print detailed logs (1=True, 0=False) |
| `--preprint` | Evaluation frequency (in epochs) |

---

## Output Files

- **Logs**: `./log/alg-dataset-lr-workers-batch-epochs-lr_decay.txt`
- **Checkpoints**: `./checkpoint/ckpt-alg-lr-extname-alpha_value-timestamp/`
- **Plots**: `./plot/alg-dataset-...-timestamp.npy` (contains accuracy/loss arrays)
- **Models**: `./model/model-alg-...-timestamp.pth`

---

## Notes

1. **LoRA Usage**: When `--lora 1`, only LoRA parameters are trainable by default
2. **Pretrained Models**: Automatically downloads required pretrained weights
3. **Data Splitting**: Uses Dirichlet distribution for non-IID splits when `--alpha_value < 1`
4. **Memory**: Adjust `--num_gpus_per` based on your GPU memory capacity

For transformer training with GLUE tasks, use `new_llm.py` with appropriate `--data_name` (QQP, MNLI, SST2, etc.).


# ðŸŒŒ **è”é‚¦å­¦ä¹ å®žéªŒå¹³å° Â· ä¸­æ–‡æ–‡æ¡£**  
*ï¼ˆæ”¯æŒ CNN & Transformer åŒæ ˆè®­ç»ƒï¼‰*

---

## ðŸ“‚ ä¸€é”®å®‰è£…ä¾èµ–
```bash
# åŸºç¡€çŽ¯å¢ƒ
pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 -i https://pypi.tuna.tsinghua.edu.cn/simple

# è”é‚¦å­¦ä¹  & æ—¥å¿—
pip install ray==1.0.0 tensorboardX==2.6.2.2 tqdm==4.67.1 -i https://pypi.tuna.tsinghua.edu.cn/simple

# Transformer & æ•°æ®é›†
pip install transformers==4.46.3 datasets==3.1.0 peft==0.13.2 -i https://pypi.tuna.tsinghua.edu.cn/simple

# ç§‘å­¦è®¡ç®—
pip install scikit-learn==1.3.2 scipy==1.9.3 matplotlib==3.7.5 -i https://pypi.tuna.tsinghua.edu.cn/simple
```

---

## ðŸŽ¯ **CNN è®­ç»ƒç¤ºä¾‹ï¼ˆCIFAR-100ï¼‰**
### 1. Swin-Tiny è”é‚¦è®­ç»ƒ
```bash
python new_adamw.py \
  --alg FedAdamW \
  --lr 3e-4 \
  --data_name CIFAR100 \
  --alpha_value 0.1 \
  --alpha 0.01 \
  --epoch 101 \
  --extname Swin_CIFAR100 \
  --lr_decay 2 \
  --gamma 0.5 \
  --CNN swin_tiny \
  --E 5 \
  --batch_size 16 \
  --gpu 2 \
  --p 1 \
  --num_gpus_per 0.2 \
  --normalization BN \
  --selection 0.05 \
  --pre 1 \
  --num_workers 100 \
  --K 50
```

### 2. ResNet-18 è”é‚¦è®­ç»ƒ
```bash
python new_adamw.py \
  --alg FedAdamW \
  --lr 3e-4 \
  --data_name CIFAR100 \
  --alpha_value 0.1 \
  --alpha 0.001 \
  --epoch 301 \
  --extname ResNet18_CIFAR100 \
  --lr_decay 2 \
  --gamma 0.5 \
  --CNN resnet18 \
  --E 5 \
  --batch_size 50 \
  --gpu 1 \
  --pix 32 \
  --lora 0 \
  --K 50
```

---

## ðŸ¤– **å¤§è¯­è¨€æ¨¡åž‹è®­ç»ƒç¤ºä¾‹ï¼ˆRoBERTa-base + GLUE-SST2ï¼‰**
```bash
python new_llm.py \
  --alg FedAdamW \
  --lr 3e-4 \
  --data_name sst2 \
  --alpha_value 0.8 \
  --alpha 0.9 \
  --epoch 101 \
  --extname RoBERTa_SST2 \
  --lr_decay 2 \
  --gamma 0.9 \
  --CNN roberta_base \
  --E 10 \
  --batch_size 32 \
  --gpu 0 \
  --p 1 \
  --num_gpus_per 0.25 \
  --selection 0.2 \
  --pre 1 \
  --num_workers 20 \
  --preprint 5 \
  --K 50 \
  --freeze 1 \
  --beta1 0.9 \
  --beta2 0.999 \
  --r 16 \
  --lora 1 \
  --print 1
```

---

## ðŸŽ›ï¸ **å‚æ•°é€ŸæŸ¥è¡¨ï¼ˆä¸­æ–‡ï¼‰**

| å‚æ•° | è¯´æ˜Ž | æŽ¨èå€¼ |
|------|------|--------|
| `--alg` | è”é‚¦ç®—æ³• | `FedAdamW`ï¼ˆAdamä¼˜åŒ–ï¼‰ / `FedAvg`ï¼ˆSGDä¼˜åŒ–ï¼‰ |
| `--lr` | å®¢æˆ·ç«¯å­¦ä¹ çŽ‡ | `3e-4`ï¼ˆAdamï¼‰ / `0.1`ï¼ˆSGDï¼‰ |
| `--data_name` | æ•°æ®é›† | `CIFAR100` / `sst2` / `qnli` |
| `--alpha_value` | æ•°æ®å¼‚æž„åº¦ | `1.0`ï¼ˆIIDï¼‰ / `0.1`ï¼ˆé«˜åº¦éžIIDï¼‰ |
| `--CNN` | æ¨¡åž‹æž¶æž„ | `swin_tiny` / `resnet18` / `roberta_base` |
| `--lora` | LoRAå¾®è°ƒ | `1`ï¼ˆå¯ç”¨ï¼‰â†’æ˜¾å­˜å ç”¨é™ä½Ž90% |
| `--r` | LoRAç§© | `16`ï¼ˆå¹³è¡¡æ€§èƒ½ä¸Žæ•ˆçŽ‡ï¼‰ |
| `--gpu` | GPUè®¾å¤‡ | `"0"`ï¼ˆå•å¡ï¼‰ / `"0,1,2"`ï¼ˆå¤šå¡ï¼‰ |
| `--epoch` | é€šä¿¡è½®æ•° | `100`ï¼ˆCNNï¼‰ / `50`ï¼ˆå¤§æ¨¡åž‹ï¼‰ |
| `--E` | æœ¬åœ°è½®æ•° | `5`ï¼ˆCNNï¼‰ / `10`ï¼ˆå¤§æ¨¡åž‹ï¼‰ |
| `--K` | æœ¬åœ°æ­¥æ•°ä¸Šé™ | è¦†ç›–`--E`çš„æ­¥æ•°é™åˆ¶ |
| `--selection` | æ¯è½®å‚ä¸Žæ¯”ä¾‹ | `0.1`ï¼ˆ10%å®¢æˆ·ç«¯ï¼‰ |
| `--batch_size` | æœ¬åœ°æ‰¹æ¬¡å¤§å° | `16`ï¼ˆGPUæ˜¾å­˜ç´§å¼ æ—¶ï¼‰ |

---

## ðŸ“Š **è¾“å‡ºæ–‡ä»¶ç»“æž„**
```
å®žéªŒç»“æžœ/
â”œâ”€â”€ log/              # è®­ç»ƒæ—¥å¿—ï¼ˆtxtï¼‰
â”œâ”€â”€ plot/             # è®­ç»ƒæ›²çº¿ï¼ˆnpyï¼‰
â”œâ”€â”€ model/            # æœ€ç»ˆæƒé‡ï¼ˆpthï¼‰
â””â”€â”€ checkpoint/       # æ–­ç‚¹ç»­è®­ï¼ˆckptï¼‰
```

---

## ðŸ’¡ **å®žç”¨æŠ€å·§**
1. **æ˜¾å­˜ä¼˜åŒ–**ï¼š  
   - CNNè®­ç»ƒï¼š`--batch_size 16` + `--num_gpus_per 0.2`  
   - å¤§æ¨¡åž‹ï¼š`--lora 1` + `--r 8`ï¼ˆæ˜¾å­˜å ç”¨ < 4GBï¼‰

2. **æ•°æ®å¼‚æž„å¯è§†åŒ–**ï¼š  
   ä¿®æ”¹`--alpha_value`ï¼ˆ0.1~1.0ï¼‰è§‚å¯Ÿç²¾åº¦å˜åŒ–æ›²çº¿

3. **å¿«é€ŸéªŒè¯**ï¼š  
   æ·»åŠ `--print 1`å®žæ—¶æŸ¥çœ‹lossï¼Œå‡å°‘`--epoch`è‡³`20`

---

## ðŸŒˆ **æ”¯æŒçš„å®Œæ•´ä»»åŠ¡åˆ—è¡¨**

| ä»»åŠ¡ç±»åž‹ | æ•°æ®é›† | æ¨¡åž‹ | ç¤ºä¾‹å‘½ä»¤ |
|----------|--------|------|----------|
| **å›¾åƒåˆ†ç±»** | CIFAR-10/100 | ResNet/Swin/DeiT | è§ä¸Šæ–¹CNNç¤ºä¾‹ |
| **æ–‡æœ¬åˆ†ç±»** | SST-2ï¼ˆæƒ…æ„Ÿåˆ†æžï¼‰ | RoBERTa-base | è§ä¸Šæ–¹LLMç¤ºä¾‹ |
| **è‡ªç„¶è¯­è¨€æŽ¨ç†** | QNLI/MNLI | RoBERTa-base | æ›¿æ¢`--data_name qnli` |
| **å¥å­å¯¹åŒ¹é…** | MRPC/QQP | RoBERTa-base | æ›¿æ¢`--data_name mrpc` |

---

**ðŸŽ‰ ç¥å®žéªŒé¡ºåˆ©ï¼æœ‰ä»»ä½•é—®é¢˜æ¬¢è¿ŽæIssueäº¤æµ~**
